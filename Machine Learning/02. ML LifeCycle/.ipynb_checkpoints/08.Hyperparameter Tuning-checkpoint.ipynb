{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "210f9380",
   "metadata": {},
   "source": [
    "#### Logistic Regression : \n",
    "\n",
    "    Default Hyperparameter Tuning : \n",
    "        linear_model.LogisticRegression(\n",
    "            penalty='l2',\n",
    "            *,\n",
    "            dual=False,\n",
    "            tol=0.0001,\n",
    "            C=1.0,\n",
    "            fit_intercept=True,\n",
    "            intercept_scaling=1,\n",
    "            class_weight=None,\n",
    "            random_state=None,\n",
    "            solver='lbfgs',\n",
    "            max_iter=100,\n",
    "            multi_class='auto',\n",
    "            verbose=0,warm_start=False,\n",
    "            n_jobs=None,\n",
    "            l1_ratio=None\n",
    "        )\n",
    "    \n",
    "\n",
    "```python\n",
    "logreg_params = {\n",
    "    'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C' : np.logspace(-4, 4, 20),\n",
    "    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n",
    "}\n",
    "```\n",
    "    Specify the norm of the penalty:\n",
    "\n",
    "        None: no penalty is added;\n",
    "    \n",
    "        'l2': add a L2 penalty term and it is the default choice;\n",
    "    \n",
    "        'l1': add a L1 penalty term;\n",
    "    \n",
    "        'elasticnet': both L1 and L2 penalty terms are added\n",
    "\n",
    "    C : float, default=1.0(The lambda value for lasso and ridge regression)\n",
    "        Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "\n",
    "    Supported penalties by solver:\n",
    "\n",
    "        ‘lbfgs’ - [‘l2’, None]\n",
    "\n",
    "        ‘liblinear’ - [‘l1’, ‘l2’]\n",
    "\n",
    "        ‘newton-cg’ - [‘l2’, None]\n",
    "\n",
    "        ‘newton-cholesky’ - [‘l2’, None]\n",
    "\n",
    "        ‘sag’ - [‘l2’, None]\n",
    "\n",
    "        ‘saga’ - [‘elasticnet’, ‘l1’, ‘l2’, None]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc568220",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes :\n",
    "\n",
    "    Gaussian Naive Bayes (GNB) is a probabilistic algorithm that makes assumptions about the distribution of the data. While it doesn't have many hyperparameters compared to some other models, you can still tune parameters like var_smoothing. \n",
    "\n",
    "    Default Hyperparameter Tuning :\n",
    "        class sklearn.naive_bayes.GaussianNB(\n",
    "            *, \n",
    "            priors=None, \n",
    "            var_smoothing=1e-09\n",
    "        )\n",
    "        \n",
    "```python\n",
    "\n",
    "gnb_param = {\n",
    "    'var_smoothing': np.logspace(0, -9, num=100)\n",
    "}\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e810b",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) :\n",
    "\n",
    "    Default Hyperparameter Tuning :\\\n",
    "        class sklearn.neighbors.KNeighborsClassifier(\n",
    "            n_neighbors=5, \n",
    "            *, \n",
    "            weights='uniform', \n",
    "            algorithm='auto', \n",
    "            leaf_size=30, \n",
    "            p=2, \n",
    "            metric='minkowski', \n",
    "            metric_params=None, \n",
    "            n_jobs=None\n",
    "        )\n",
    "```python\n",
    "\n",
    "knn_param = {\n",
    "    'n_neighbors': np.arange(1, 21),  # Try values from 1 to 20\n",
    "    'weights': ['uniform', 'distance'],  # Try different weight options\n",
    "    'p': [1, 2]  # Try Manhattan (p=1) and Euclidean (p=2) distances\n",
    "}\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95a767",
   "metadata": {},
   "source": [
    "#### Decision Trees :\n",
    "\n",
    "    Default Hyperparameter Tuning :\n",
    "        class sklearn.tree.DecisionTreeClassifier(\n",
    "            *, \n",
    "            criterion='gini', \n",
    "            splitter='best', \n",
    "            max_depth=None, \n",
    "            min_samples_split=2, \n",
    "            min_samples_leaf=1, \n",
    "            min_weight_fraction_leaf=0.0, \n",
    "            max_features=None, \n",
    "            random_state=None, \n",
    "            max_leaf_nodes=None, \n",
    "            min_impurity_decrease=0.0, \n",
    "            class_weight=None, \n",
    "            ccp_alpha=0.0\n",
    "        )\n",
    "\n",
    "```python\n",
    "dt_param = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2474784f",
   "metadata": {},
   "source": [
    "#### Random Forest :\n",
    "\n",
    "    Default Hyperparameter Tuning :\n",
    "        class sklearn.ensemble.RandomForestClassifier(\n",
    "            n_estimators=100, \n",
    "            *, \n",
    "            criterion='gini', \n",
    "            max_depth=None, \n",
    "            min_samples_split=2, \n",
    "            min_samples_leaf=1, \n",
    "            min_weight_fraction_leaf=0.0, \n",
    "            max_features='sqrt', \n",
    "            max_leaf_nodes=None, \n",
    "            min_impurity_decrease=0.0, \n",
    "            bootstrap=True, \n",
    "            oob_score=False, \n",
    "            n_jobs=None, \n",
    "            random_state=None, \n",
    "            verbose=0, \n",
    "            warm_start=False, \n",
    "            class_weight=None, \n",
    "            ccp_alpha=0.0, \n",
    "            max_samples=None\n",
    "        )\n",
    "\n",
    "```python\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f32c6fa",
   "metadata": {},
   "source": [
    "#### Support Vector Machines (SVM) :\n",
    "\n",
    "    Default Hyperparameter Tuning : \n",
    "        class sklearn.svm.SVC(\n",
    "            *, \n",
    "            C=1.0, \n",
    "            kernel='rbf', \n",
    "            degree=3, \n",
    "            gamma='scale', \n",
    "            coef0=0.0, \n",
    "            shrinking=True, \n",
    "            probability=False, \n",
    "            tol=0.001, \n",
    "            cache_size=200, \n",
    "            class_weight=None, \n",
    "            verbose=False, \n",
    "            max_iter=-1, \n",
    "            decision_function_shape='ovr', \n",
    "            break_ties=False, \n",
    "            random_state=None\n",
    "        )\n",
    "        \n",
    "```python\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly','rbf', 'sigmoid'],    \n",
    "    'gamma': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "OR\n",
    "\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly','rbf', 'sigmoid'],    \n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
